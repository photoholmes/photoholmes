# Pre-Processing

## Overview

Most methods don't take the full image as an input, but rather transformation of it
(grayscale, dct volumes, etc...).  We identified some of this transformation and
packaged them in reusable modules, and implemented PreProcessing pipelines to group
preprocessing sequences for the methods.

## Handling images

Within the photoholmes framework exist methods that handle images as tensors and
other that handle images as numpy arrays. To simplify the process of implementing and debugging methods, we use the following convention of dimension for each type:

- **Numpy array:** [H, W, C]
- **Torch Tensor:** [C, H, W]

## Available transformations

This is the list of available transformations:

1. **ZeroOneRange:** changes the image range from [0, 255] to [0, 1]
2. **Normalize:** takes the image to 0 mean and variance 1.
3. **RGBtoGray:** converts an RGB image to grayscale.
4. **GrayToRGB:** converts an grayscale image to RGB.
5. **RoundToUInt:** rounds the input  
6. **RGBtoYCrCB:** converts RGB image to YCrCb.
7. **GetImageSize:** add image_size to the pipeline output. Be sure to add image_size
to your [pipeline output keys](#output-keys).
8. **ToNumpy:** converts an image (torch Tensor or PIL image) to a numpy array.
9. **ToTensor:** converts an image (numpy array or PIL image) to a torch Tensor.

## Using a pipeline

The pipeline will output a dictionary with the keys defined in the `output_keys` attribute.
These `output_keys` should match the method input keys, so you can feed the output directly to the method by using the `**` operator. Here's an example of how to use the pipeline:

```python
from photoholmes.methods.dq import DQ, dq_preprocessing
from photoholmes.utils.image import read_image, read_jpeg_data

# load image
image_path = "<path_to_image>"
image = read_image(image_path)
dct_coefficients, _ = read_jpeg_data(image_path)

# create method
dq = DQ()

method_input = dq_preprocessing(image=image, dct_coefficients=dct_coefficients)
heatmap = dq.predict(**method_input)
```

## Creating a pipeline

To run a sequence of transforms on an image, photoholmes provides the `PreProcessingPipeline` object. Let's create a pipeline that takes an image, converts
it to a grayscale, divides it by 255 and normalizes it:

```python
from photoholmes.preprocessing import (
    PreProcessingPipeline,
    RGBtoGray,
    Normalize,
    ZeroOneRange,
)

pipeline = PreProcessingPipeline(
    transforms=[RGBtoGray(), Normalize(), ZeroOneRange()],
    inputs=["image"],
    outputs_keys=["image"],
)
```

When creating a pipeline, on top of the transforms you want to add, you will need to 
define two extra parameters: _inputs_ and _outputs_keys_.

### Inputs

`inputs` serves two purposes: to validate the input to the pipeline when called (useful for debugging), and for the [Dataset](../datasets/README.md) to know what info it should load from the image and which isn't necessary. Currently, we limit the inputs to:

- image
- dct_coefficients
- qtables

If you provide an extra input beside this, it will pass throught the pipeline, since
all the transforms allow for passthrough values.

### Output keys

The `output_keys` attribute is a filter to remove anything the model doesn't need to run a predict. Take Catnet for example. Catnet expects a tensor `x` and the qtables. The
tensor `x` is a concatenation of the image with matrices generated by a custom preprocessing
from the dct coefficients.
As such, it has `image`, `dct_coefficients` and `qtables` as inputs, yet it only has to
output `x` and `qtables` for the model to use.

## Creating a custom transform

If your method requires a custom transformation of the image, you can create a custom
transform by subclassing the `PreProcessingTransform` class. Let's take AdaptiveCFANet
as an example. The network requires the input image to have even dimensions. We can create a custom transform to handle this requirement:

```python
class AdaptiveCFANetPreprocessing(BasePreprocessing):
    def __init__(self):
        pass

    def __call__(
        self,
        image: Tensor,
        **kwargs: Dict[str, Any],
    ) -> Dict[str, Any]:
        """
        Adjusts the input image dimensions to ensure they are even, as required
        by the AdaptiveCFANet architecture. This is done by trimming the image
        to the largest even dimensions that are smaller than or equal to the
        original dimensions.

        Args:
            image (Tensor): The input image tensor with shape (C, Y, X), where
                C is the number of channels, Y is the height, and X is the width.
            **kwargs (Dict[str, Any]): Additional keyword arguments that might
                be passed to the preprocessing function and need to be preserved in the
                output.

        Returns:
            Dict[str, Any]: A dictionary containing the preprocessed image tensor
                under the key 'image', along with any other keyword arguments passed
                into the method.
        """

        C, Y_o, X_o = image.shape
        image = image[:C, : Y_o - Y_o % 2, : X_o - X_o % 2]

        return {"image": image, **kwargs}
```

## Contributing: Adding a new transform

If you want to add a new transform, please follow the following steps:

1. Create a new file or modify an existing one in the `photoholmes/preprocessing/` directory.
2. Create a new class that subclasses `BasePreprocessing`.
3. Implement the `__call__` method, which should take the input image and any other necessary arguments, and return a dictionary containing the preprocessed image and any other necessary arguments.
4. Add the new transform to the `__init__.py` file in the `photoholmes/preprocessing/` directory.
5. Add a test for the new transform in the `tests/preprocessing/test_transforms.py` file.
6. Run the tests to ensure that the new transform works as expected.
7. Create a pull request with the new transform and tests.
8. Once the pull request is approved, merge it into the main branch.
